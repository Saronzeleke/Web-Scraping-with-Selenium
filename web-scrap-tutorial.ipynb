{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" Web Scraping with Selenium: From Static to Dynamic Pages\n","metadata":{}},{"cell_type":"markdown","source":"1. Introduction to Web Scraping Web scraping is the process of extracting data from websites automatically.\nWe use it when:\n\n   - APIs are not available or limited\n\n   - Data needs to be collected from multiple sources\n\n   - Real-time data extraction is required","metadata":{}},{"cell_type":"markdown","source":"Types of Pages:\n\nA. Static Pages: Content is fixed and doesn't change after loading\n\nB. Dynamic Pages: Content changes based on user interactions (JavaScript)","metadata":{}},{"cell_type":"markdown","source":"**2. Selenium Setup and Installation\nFirst, let's install the required packages:**","metadata":{}},{"cell_type":"code","source":"pip install selenium beautifulsoup4 pandas webdriver-manager","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"we'll use Wikipedia as our example website.","metadata":{}},{"cell_type":"markdown","source":"3. Basic Selenium Concepts\nWebDriver Setup","metadata":{}},{"cell_type":"code","source":"from selenium import webdriver\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nfrom selenium.common.exceptions import NoSuchElementException, TimeoutException\nimport time\nimport pandas as pd\n\n# Setup WebDriver with automatic management\ndef setup_driver():\n    options = webdriver.ChromeOptions()\n    options.add_argument('--headless')  \n    options.add_argument('--no-sandbox')\n    options.add_argument('--disable-dev-shm-usage')\n    \n    service = Service(ChromeDriverManager().install())\n    driver = webdriver.Chrome(service=service, options=options)\n    return driver\n\n# Initialize driver\ndriver = setup_driver()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Key Selenium Methods\ndriver.get(url) - Navigate to URL\n\ndriver.find_element(By.*, \"selector\") - Find single element\n\ndriver.find_elements(By.*, \"selector\") - Find multiple elements\n\nelement.click() - Click on element\n\nelement.send_keys(\"text\") - Send text to input field\n\ndriver.back(), driver.forward() - Navigation","metadata":{}},{"cell_type":"markdown","source":"4. Static Page Scraping - Wikipedia Example\nStatic pages are easier to scrape as content loads immediately.","metadata":{}},{"cell_type":"code","source":"def scrape_wikipedia_static():\n    \"\"\"Scrape static content from Wikipedia main page\"\"\"\n    try:\n        # Navigate to Wikipedia\n        driver.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n        \n        # Wait for page to load\n        WebDriverWait(driver, 10).until(\n            EC.presence_of_element_located((By.ID, \"mp-tfa\"))\n        )\n        \n        # Extract featured article\n        featured_article = driver.find_element(By.ID, \"mp-tfa\")\n        article_title = featured_article.find_element(By.TAG_NAME, \"p\").text\n        article_link = featured_article.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n        \n        print(\"Featured Article:\", article_title)\n        print(\"Link:\", article_link)\n        \n        # Extract news headlines\n        news_section = driver.find_element(By.ID, \"mp-itn\")\n        news_items = news_section.find_elements(By.TAG_NAME, \"li\")\n        \n        news_data = []\n        for item in news_items:\n            news_data.append({\n                'text': item.text,\n                'link': item.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n            })\n        \n        return {\n            'featured_article': {'title': article_title, 'link': article_link},\n            'news_items': news_data\n        }\n        \n    except Exception as e:\n        print(f\"Error scraping static content: {e}\")\n        return None\n\n# Execute static scraping\nstatic_data = scrape_wikipedia_static()\nprint(\"Static Scraping Results:\")\nprint(static_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"5. Dynamic Page Scraping - Wikipedia Search\nDynamic content requires waiting for elements to load or interactions.","metadata":{}},{"cell_type":"code","source":"def scrape_wikipedia_dynamic(search_term=\"Data science\"):\n    \"\"\"Scrape dynamic content using Wikipedia search\"\"\"\n    try:\n        # Navigate to Wikipedia\n        driver.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n        \n        # Find search box and enter search term\n        search_box = WebDriverWait(driver, 10).until(\n            EC.element_to_be_clickable((By.ID, \"searchInput\"))\n        )\n        search_box.clear()\n        search_box.send_keys(search_term)\n        \n        # Click search button\n        search_button = driver.find_element(By.ID, \"searchButton\")\n        search_button.click()\n        \n        # Wait for search results to load\n        WebDriverWait(driver, 10).until(\n            EC.presence_of_element_located((By.ID, \"firstHeading\"))\n        )\n        \n        # Extract page title\n        page_title = driver.find_element(By.ID, \"firstHeading\").text\n        \n        # Extract content paragraphs\n        content_paragraphs = driver.find_elements(\n            By.CSS_SELECTOR, \"#mw-content-text .mw-parser-output > p\"\n        )\n        \n        # Extract table of contents (dynamic element)\n        toc_section = driver.find_element(By.ID, \"toc\")\n        toc_items = toc_section.find_elements(By.TAG_NAME, \"li\")\n        \n        toc_data = []\n        for item in toc_items:\n            toc_data.append(item.text)\n        \n        # Extract references (might load dynamically)\n        references = driver.find_elements(\n            By.CSS_SELECTOR, \".references li\"\n        )\n        \n        ref_data = []\n        for ref in references[:5]:  \n            ref_data.append(ref.text)\n        \n        return {\n            'search_term': search_term,\n            'page_title': page_title,\n            'content_preview': content_paragraphs[0].text if content_paragraphs else \"\",\n            'toc': toc_data,\n            'references_sample': ref_data\n        }\n        \n    except Exception as e:\n        print(f\"Error scraping dynamic content: {e}\")\n        return None\n\n# Execute dynamic scraping\ndynamic_data = scrape_wikipedia_dynamic(\"Machine learning\")\nprint(\"Dynamic Scraping Results:\")\nprint(dynamic_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"6. Best Practices and Ethics\n\n   \n","metadata":{}},{"cell_type":"markdown","source":"1. Respect robots.txt","metadata":{}},{"cell_type":"code","source":"def check_robots_txt(url):\n    \"\"\"Check if scraping is allowed\"\"\"\n    import requests\n    robots_url = f\"{url}/robots.txt\"\n    try:\n        response = requests.get(robots_url)\n        print(\"Robots.txt content:\")\n        print(response.text)\n    except:\n        print(\"Could not fetch robots.txt\")\n\ncheck_robots_txt(\"https://en.wikipedia.org\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. Rate Limiting and Delays","metadata":{}},{"cell_type":"code","source":"import random\nimport time\n\ndef respectful_delay():\n    \"\"\"Add random delay between requests\"\"\"\n    time.sleep(random.uniform(1, 3))\n\ndef scrape_with_respect(url):\n    \"\"\"Scrape with respect to website resources\"\"\"\n    respectful_delay()\n    driver.get(url)\n    # Your scraping code here","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. Error Handling","metadata":{}},{"cell_type":"code","source":"def robust_scraping():\n    \"\"\"Scraping with comprehensive error handling\"\"\"\n    try:\n        # Your scraping code\n        pass\n    except NoSuchElementException:\n        print(\"Element not found\")\n    except TimeoutException:\n        print(\"Page loading timeout\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n    finally:\n        driver.quit()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"7. Complete  Our Project\n\n   \nLet's create a complete Wikipedia scraper that handles both static and dynamic content:","metadata":{}},{"cell_type":"code","source":"class WikipediaScraper:\n    def __init__(self):\n        self.driver = setup_driver()\n        self.wait = WebDriverWait(self.driver, 10)\n    \n    def scrape_article(self, article_title):\n        \"\"\"Complete article scraping with metadata\"\"\"\n        try:\n            # Navigate directly to article\n            url = f\"https://en.wikipedia.org/wiki/{article_title.replace(' ', '_')}\"\n            self.driver.get(url)\n            \n            # Wait for page to load\n            self.wait.until(EC.presence_of_element_located((By.ID, \"firstHeading\")))\n            \n            # Extract basic info\n            title = self.driver.find_element(By.ID, \"firstHeading\").text\n            content = self.driver.find_element(By.ID, \"mw-content-text\").text[:500] + \"...\"\n            \n            # Extract infobox (if exists)\n            infobox_data = self._extract_infobox()\n            \n            # Extract categories\n            categories = self._extract_categories()\n            \n            # Extract external links\n            external_links = self._extract_external_links()\n            \n            return {\n                'title': title,\n                'url': url,\n                'content_preview': content,\n                'infobox': infobox_data,\n                'categories': categories,\n                'external_links': external_links\n            }\n            \n        except Exception as e:\n            print(f\"Error scraping article: {e}\")\n            return None\n    \n    def _extract_infobox(self):\n        \"\"\"Extract data from infobox\"\"\"\n        try:\n            infobox = self.driver.find_element(By.CLASS_NAME, \"infobox\")\n            rows = infobox.find_elements(By.TAG_NAME, \"tr\")\n            \n            data = {}\n            for row in rows:\n                try:\n                    th = row.find_element(By.TAG_NAME, \"th\")\n                    td = row.find_element(By.TAG_NAME, \"td\")\n                    data[th.text] = td.text\n                except:\n                    continue\n            \n            return data\n        except:\n            return {}\n    \n    def _extract_categories(self):\n        \"\"\"Extract article categories\"\"\"\n        try:\n            cat_links = self.driver.find_elements(\n                By.CSS_SELECTOR, \"#mw-normal-catlinks ul li a\"\n            )\n            return [link.text for link in cat_links]\n        except:\n            return []\n    \n    def _extract_external_links(self):\n        \"\"\"Extract external references\"\"\"\n        try:\n            ext_links = self.driver.find_elements(\n                By.CSS_SELECTOR, \".external.text\")\n            return [link.get_attribute(\"href\") for link in ext_links[:5]]\n        except:\n            return []\n    \n    def search_and_scrape(self, search_term):\n        \"\"\"Search Wikipedia and scrape first result\"\"\"\n        try:\n            # Perform search\n            self.driver.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n            search_box = self.wait.until(\n                EC.element_to_be_clickable((By.ID, \"searchInput\"))\n            )\n            search_box.clear()\n            search_box.send_keys(search_term)\n            search_box.submit()\n            \n            # Wait for results and click first result\n            self.wait.until(\n                EC.presence_of_element_located((By.CSS_SELECTOR, \".mw-search-results li\"))\n            )\n            first_result = self.driver.find_element(\n                By.CSS_SELECTOR, \".mw-search-results li a\"\n            )\n            article_url = first_result.get_attribute(\"href\")\n            \n            # Scrape the article\n            return self.scrape_article(article_url.split('/')[-1])\n            \n        except Exception as e:\n            print(f\"Error in search and scrape: {e}\")\n            return None\n    \n    def close(self):\n        \"\"\"Clean up\"\"\"\n        self.driver.quit()\n\n# Usage example\ndef main():\n    scraper = WikipediaScraper()\n    \n    # Example 1: Direct article scraping\n    print(\"Scraping 'Data Science' article:\")\n    data_science_data = scraper.scrape_article(\"Data science\")\n    print(data_science_data)\n    \n    # Example 2: Search and scrape\n    print(\"\\nSearching for 'Artificial Intelligence':\")\n    ai_data = scraper.search_and_scrape(\"Artificial Intelligence\")\n    print(ai_data)\n    \n    # Example 3: Multiple articles\n    topics = [\"Machine Learning\", \"Python (programming language)\", \"Web scraping\"]\n    results = []\n    \n    for topic in topics:\n        print(f\"\\nScraping: {topic}\")\n        result = scraper.scrape_article(topic)\n        if result:\n            results.append(result)\n        respectful_delay()\n    \n    # Save to CSV\n    df = pd.DataFrame(results)\n    df.to_csv(\"wikipedia_data.csv\", index=False)\n    print(\"\\nData saved to wikipedia_data.csv\")\n    \n    scraper.close()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Key Takeaways:\n\nStatic vs Dynamic: Understand when content loads immediately vs when it requires interaction\n\nWaiting Strategies: Use explicit waits for dynamic content loading\n\nError Handling: Always implement robust error handling\n\nEthical Scraping: Respect robots.txt, add delays, and don't overload servers\n\nData Organization: Structure your scraped data for easy analysis","metadata":{}}]}